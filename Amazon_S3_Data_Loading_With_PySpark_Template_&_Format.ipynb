{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Amazon_S3_Data_Loading_With_PySpark_Template_&_Format.ipynb","provenance":[],"authorship_tag":"ABX9TyNaiPGryHoQ4eDIC53BJP4z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_jzS2DclfF42"},"outputs":[],"source":["# Amazon S3 Data Loading url format - U.S. East (default region)\n","\n","template_url = \"https://<bucket-name>.s3.amazonaws.com/<folder-name>/<file-name>\"\n","\n","example_url = \"https://dataviz-curriculum.s3.amazonaws.com/data-folder/data.csv\"\n"]},{"cell_type":"code","source":["# Other Regions [GENERALLY IGNOREABLE]\n","\n","template_url = \"https://<bucket-name.s3-<region>.amazonaws.com/<folder-name>/<file-name>\"\n","\n","example_url =\" https://dataviz-curriculum.s3-us-west-1.amazonaws.com/data-folder/data.csv\""],"metadata":{"id":"OOj6exApfSnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mif0wNWcff__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Basics for Reading Amazon S3 Data into our workspace with PySpark"],"metadata":{"id":"okU_2-HKfhjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HVGh5dnAfomf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install packages not native to Google CoLab - PySpark & Java\n","\n","import os\n","# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.<enter version>'\n","spark_version = 'spark-3.2.1'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"],"metadata":{"id":"Cjd0gpS-foyk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start Spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Yelp_NLP\").getOrCreate()"],"metadata":{"id":"w-2VAzdpfqmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read in data from Amazon S3 Buckets\n","from pyspark import SparkFiles\n","url =\"https://<bucket-name>.s3.amazonaws.com/<folder-name>/<file-name>\"\n","spark.sparkContext.addFile(url)\n","df = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n","\n","# Show DataFrame\n","df.show()"],"metadata":{"id":"cjzA14Idfsi4"},"execution_count":null,"outputs":[]}]}